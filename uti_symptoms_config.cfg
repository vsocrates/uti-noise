[paths]
train = null
dev = null
vectors = null
init_tok2vec = null #~/Documents/prodigy/tok2vec

[system]
seed = 1234
gpu_allocator = "pytorch"

[nlp]
lang = "en"
# List of pipeline component names, in order. The names should correspond to
# components defined in the [components block]
pipeline = ["transformer", "ner"]
# Components that are loaded but disabled by default
disabled = [ 'tagger', 'parser', 'attribute_ruler','lemmatizer',]
# Optional callbacks to modify the nlp object before it's initialized, after
# it's created and after the pipeline has been set up
before_creation = null
after_creation = null
after_pipeline_creation = null
# Default batch size to use with nlp.pipe and nlp.evaluate
batch_size = 2


[components]

[components.transformer]
max_batch_items = 64

# [components.transformer.set_extra_annotations]
# @annotation_setters = "spacy-transformers.null_annotation_setter.v1"

[components.transformer.model]
# @architectures = "spacy-transformers.TransformerModel.v3"
# name = "roberta-base"
# tokenizer_config = {"use_fast": true}
# transformer_config = {}
# mixed_precision = false
# grad_scaler_config = {}

[components.transformer.model.get_spans]
@span_getters = "spacy-transformers.strided_spans.v1"
window = 128
stride = 96

[corpora]

[corpora.train]
# Limitations on training document length
max_length = 128
# Limitation on number of training examples
limit = 5

[corpora.dev]
# Limitations on training document length
max_length = 128
# Limitation on number of training examples
limit = 5

# Training hyper-parameters and additional features.
[training]
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
# Controls early-stopping, i.e., the number of steps to continue without
# improvement before stopping. 0 disables early stopping.
patience = 500
# Number of epochs. 0 means unlimited. If >= 0, train corpus is loaded once in
# memory and shuffled within the training loop. -1 means stream train corpus
# rather than loading in memory with no shuffling within the training loop.
# max_epochs = 0
# Maximum number of update steps to train for. 0 means an unlimited number of steps.
max_steps = 8000 #10000
eval_frequency = 200
# Control how scores are printed and checkpoints are evaluated.
# score_weights = {}
# Names of pipeline components that shouldn't be updated during training
# frozen_components = []
# Names of pipeline components that should set annotations during training
# annotating_components = []
# Location in the config where the dev corpus is defined
# dev_corpus = "corpora.dev"
# Location in the config where the train corpus is defined
# train_corpus = "corpora.train"
# Optional callback before nlp object is saved to disk after training
before_to_disk = null

[training.logger]
@loggers = "spacy.ConsoleLogger.v1"

[training.batcher]
@batchers = "spacy.batch_by_padded.v1"
discard_oversize = true
size = 50
buffer = 128
get_length = null

# [training.batcher]
# @batchers = "spacy.batch_by_words.v1"
# discard_oversize = false
# tolerance = 0.2

# [training.batcher.size]
# @schedules = "compounding.v1"
# start = 100
# stop = 1000
# compound = 1.001

[training.optimizer]
@optimizers = "Adam.v1"
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 1e-8
# learn_rate = 0.0001

[training.optimizer.learn_rate]
@schedules = "warmup_linear.v1"
warmup_steps = 250
total_steps = 20000
initial_rate = 0.00005

[training.score_weights]
ents_per_type = null #"{'problem': {'p': 0,'r': 0,'f': 0.15},'organ_failure_related_to_problem': {'p': 0,'r': 0,'f': 0.35},'event_related_to_problem': {'p': 0,'r': 0,'f': 0.15}, 'complication_related_to_problem': {'p': 0,'r': 0,'f': 0.35}}"
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0

# These settings are used when nlp.initialize() is called (typically before
# training or pretraining). Components and the tokenizer can each define their
# own arguments via their initialize methods that are populated by the config.
# This lets them gather data resources, build label sets etc.
[initialize]
vectors = ${paths.vectors}
# Extra resources for transfer-learning or pseudo-rehearsal
init_tok2vec = ${paths.init_tok2vec}
# Data and lookups for vocabulary
vocab_data = null
lookups = null
# Arguments passed to the tokenizer's initialize method
tokenizer = {}
# Arguments for initialize methods of the components (keyed by component)
components = {}
before_init = null
after_init = null
